ğŸ§  Sign Language Interpreter â€“ ASL Interpreter for AR Glasses
A multi-component system that uses machine learning, computer vision, AR glasses, and natural language processing to interpret British Sign Language (BSL) in real time.

ğŸ“ Project Structure
bash
Copy
Edit
ğŸ“‚ BSL-Interpreter-AR
â”œâ”€â”€ ğŸ“‚ data          # ASL dataset & extracted keypoints
â”œâ”€â”€ ğŸ“‚ models        # Trained machine learning models
â”œâ”€â”€ ğŸ“‚ actions       # rasa run components
â”œâ”€â”€ ğŸ“‚ code      # Python & R scripts for processing
â”œâ”€â”€ ğŸ“‚ ar_display    # AR overlay for captions (Unity/OpenCV)
â”œâ”€â”€ ğŸ“‚ tests         #testing the pythin model for essay
â”œâ”€â”€ ğŸ“‚ app              # flask_app.py
â”œâ”€â”€config.yml
â”œâ”€â”€credentials.yml
â”œâ”€â”€ domain.yml
â”œâ”€â”€endpoints.yml
â”œâ”€â”€sign_language_model.h5
â”œâ”€â”€training_history.png
â”œâ”€â”€ README.md        # Project documentation

ğŸ› ï¸ Installation
1ï¸âƒ£ Install Dependencies
Ensure you have Python 3.7â€“3.10 installed.

2ï¸âƒ£ Setup Virtual Environments
The hand tracking and RASA have incompatible package requirements, so two virtual environments should be configured

# Terminal 1 (Hand Tracking)
```bash
python -m venv venv_tracking
source venv_tracking/bin/activate  # Mac/Linux
venv_tracking\Scripts\activate     # Windows
pip install mediapipe opencv-python numpy pandas tensorflow keras flask flask-cors
```
If using R for training:

install.packages("randomForest")
install.packages("keras")

# Terminal 2 (RASA)
```bash
py -3.10 -m venv venv_rasa
source venv_rasa/bin/activate  # Mac/Linux
venv_rasa\Scripts\activate     # Windows
pip install rasa pyttsx3 SpeechRecognition pyaudio
```

ğŸ¥ How It Works
ğŸ“· Step 1: Capture Video Input
Use OpenCV to access the webcam:

python
Copy
Edit
import cv2

cap = cv2.VideoCapture(0)
while cap.isOpened():
    ret, frame = cap.read()
    cv2.imshow("BSL Camera Feed", frame)
    if cv2.waitKey(1) & 0xFF == ord("q"):
        break

cap.release()
cv2.destroyAllWindows()
âœ‹ Step 2: Extract Hand & Pose Keypoints (Mediapipe)
python
Copy
Edit
import mediapipe as mp

mp_hands = mp.solutions.hands
hands = mp_hands.Hands()
ğŸ§  Step 3: Train AI Model
In R:
R
Copy
Edit
library(randomForest)
data <- read.csv("bsl_dataset.csv")
model <- randomForest(label ~ ., data = data, ntree = 200)
saveRDS(model, "bsl_rf_model.rds")
In Python:
python
Copy
Edit
from tensorflow.keras.models import Sequential

model = Sequential([...])
model.fit(X_train, y_train, epochs=50, batch_size=16)
model.save("bsl_model.h5")
ğŸ•¶ï¸ Step 4: Real-Time Captioning in AR
Overlay predictions using OpenCV or Unity:

python
Copy
Edit
cv2.putText(frame, "Hello!", (50, 50),
            cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)

ğŸ§ª Running the Project
4ï¸âƒ£ Run Virtual Environments
For testing / demo:
Just run 'python run_all.py'. This will setup everything in the correct venv.

For debugging:
Open four terminal windows. Run the tracking venv in terminal 1, then the rasa venv in terminals 2, 3 and 4.

Terminal 1: python code/hand_tracking.py
Terminal 2: rasa run actions
Terminal 3: rasa run --enable-api --cors "*" --debug
Terminal 4: python handlers/speech_handler.py 

If changing intents, retrain the model with 'rasa train' in a rasa venv.

# RASA shell
'What gesture is this?'
'Test me'

ğŸ•¶ï¸ Connecting to the AR Glasses (Unity)
To run the Unity integration:

Open the project in Unity.

Make sure launch.json is set up (for debugging).

Start flask_app.py in the tracking virtual environment:

bash
Copy
Edit
python flask_app.py
Press Play in Unity. Gesture data should now stream from Flask to Unity and display in the AR overlay.

ğŸ“Œ Challenges & Future Improvements
âŒ Improve ASL recognition accuracy with a larger dataset

âŒ Implement sentence-level sign translation

âŒ Optimise real-time AR performance for mobile/edge devices

ğŸ’¡ Contributors

Name	Role
Emma Davidson	AI/ML Specialist â€¢ Unity Developer â€¢ RASA Developer
Annie O'Boyle	Essay Writer & Documentation
Neil	Dialogue Management â€¢ RASA Integration
Sarah Jade Ruthven	Unity Developer
ğŸ“œ License
This project is licensed under the MIT License.

ğŸŒŸ Final Thoughts
Letâ€™s break communication barriers with the power of AI and AR.
Real-time. Accessible. Inclusive.
