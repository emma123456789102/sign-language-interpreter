ğŸ§  Sign Language Interpreter â€“ ASL Interpreter for AR Glasses
A multi-component system that uses machine learning, computer vision, AR glasses, and natural language processing to interpret British Sign Language (BSL) in real time.

ğŸ“ Project Structure
bash
Copy
Edit
ğŸ“‚ BSL-Interpreter-AR
â”œâ”€â”€ ğŸ“‚ data          # ASL dataset & extracted keypoints
â”œâ”€â”€ ğŸ“‚ models        # Trained machine learning models
â”œâ”€â”€ ğŸ“‚ actions       # rasa run components
â”œâ”€â”€ ğŸ“‚ code      # Python & R scripts for processing
â”œâ”€â”€ ğŸ“‚ ar_display    # AR overlay for captions (Unity/OpenCV)
â”œâ”€â”€ ğŸ“‚ tests         #testing the pythin model for essay
â”œâ”€â”€ ğŸ“‚ app              # flask_app.py
â”œâ”€â”€config.yml
â”œâ”€â”€credentials.yml
â”œâ”€â”€ domain.yml
â”œâ”€â”€endpoints.yml
â”œâ”€â”€sign_language_model.h5
â”œâ”€â”€training_history.png
â”œâ”€â”€ README.md        # Project documentation

ğŸ› ï¸ Installation
1ï¸âƒ£ Install Dependencies
Ensure you have Python 3.7â€“3.10 installed.

2ï¸âƒ£ Setup Virtual Environments
Hand tracking and RASA have incompatible dependencies, so two virtual environments are required.

Terminal 1: Hand Tracking (Python)
bash
Copy
Edit
python -m venv venv_tracking
source venv_tracking/bin/activate     # Mac/Linux
venv_tracking\Scripts\activate        # Windows

pip install mediapipe opencv-python numpy pandas tensorflow keras flask
If using R for training:
R
Copy
Edit
install.packages("randomForest")
install.packages("keras")
Terminal 2: RASA
bash
Copy
Edit
python -m venv venv_rasa
source venv_rasa/bin/activate         # Mac/Linux
venv_rasa\Scripts\activate            # Windows

pip install rasa
ğŸ¥ How It Works
ğŸ“· Step 1: Capture Video Input
Use OpenCV to access the webcam:

python
Copy
Edit
import cv2

cap = cv2.VideoCapture(0)
while cap.isOpened():
    ret, frame = cap.read()
    cv2.imshow("BSL Camera Feed", frame)
    if cv2.waitKey(1) & 0xFF == ord("q"):
        break

cap.release()
cv2.destroyAllWindows()
âœ‹ Step 2: Extract Hand & Pose Keypoints (Mediapipe)
python
Copy
Edit
import mediapipe as mp

mp_hands = mp.solutions.hands
hands = mp_hands.Hands()
ğŸ§  Step 3: Train AI Model
In R:
R
Copy
Edit
library(randomForest)
data <- read.csv("bsl_dataset.csv")
model <- randomForest(label ~ ., data = data, ntree = 200)
saveRDS(model, "bsl_rf_model.rds")
In Python:
python
Copy
Edit
from tensorflow.keras.models import Sequential

model = Sequential([...])
model.fit(X_train, y_train, epochs=50, batch_size=16)
model.save("bsl_model.h5")
ğŸ•¶ï¸ Step 4: Real-Time Captioning in AR
Overlay predictions using OpenCV or Unity:

python
Copy
Edit
cv2.putText(frame, "Hello!", (50, 50),
            cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)
ğŸ§ª Running the Project
Open three terminal windows:

Terminal 1: Activate tracking venv and run hand tracking:

bash
Copy
Edit
source venv_tracking/bin/activate
python scripts/hand_tracking.py
Terminal 2: Activate RASA venv and run actions:

bash
Copy
Edit
source venv_rasa/bin/activate
rasa run actions
Terminal 3: Run RASA shell:

bash
Copy
Edit
rasa shell
To update RASA intents, run:

bash
Copy
Edit
rasa train
ğŸ•¶ï¸ Connecting to the AR Glasses (Unity)
To run the Unity integration:

Open the project in Unity.

Make sure launch.json is set up (for debugging).

Start flask_app.py in the tracking virtual environment:

bash
Copy
Edit
python flask_app.py
Press Play in Unity. Gesture data should now stream from Flask to Unity and display in the AR overlay.

ğŸ“Œ Challenges & Future Improvements
âŒ Improve ASL recognition accuracy with a larger dataset

âŒ Implement sentence-level sign translation

âŒ Optimise real-time AR performance for mobile/edge devices

ğŸ’¡ Contributors

Name	Role
Emma Davidson	AI/ML Specialist â€¢ Unity Developer â€¢ RASA Developer
Annie O'Boyle	Essay Writer & Documentation
Neil	Dialogue Management â€¢ RASA Integration
Sarah Jade Ruthven	Unity Developer
ğŸ“œ License
This project is licensed under the MIT License.

ğŸŒŸ Final Thoughts
Letâ€™s break communication barriers with the power of AI and AR.
Real-time. Accessible. Inclusive.
