
# ğŸ§  Sign Language Interpreter â€“ ASL Interpreter for AR Glasses

A multi-component system that uses machine learning, computer vision, a webcam, and natural language processing to interpret American Sign Language (ASL) in real time.

---

## ğŸ“ Project Structure

```
sign-language-interpreter/
â”œâ”€â”€ data/               # ASL dataset & extracted keypoints
â”œâ”€â”€ models/             # Trained machine learning models
â”œâ”€â”€ actions/            # RASA action components
â”œâ”€â”€ code/               # Python & R scripts for processing
â”œâ”€â”€ ar_display/         # AR overlay for captions (Unity/OpenCV)
â”œâ”€â”€ tests/              # Python model testing for essay
â”œâ”€â”€ app/                # Flask API (flask_app.py)
â”œâ”€â”€ config.yml
â”œâ”€â”€ credentials.yml
â”œâ”€â”€ domain.yml
â”œâ”€â”€ endpoints.yml
â”œâ”€â”€ sign_language_model.h5
â”œâ”€â”€ training_history.png
â””â”€â”€ README.md           # Project documentation
```

---

## ğŸ› ï¸ Installation

### 1ï¸âƒ£ Install Dependencies

Ensure you have **Python 3.7â€“3.10** installed.

### 2ï¸âƒ£ Set Up Virtual Environments

Due to incompatible package requirements between the hand tracking module and RASA, two virtual environments are needed.

#### ğŸ”¹ Terminal 1 â€“ Hand Tracking

```bash
python -m venv venv_tracking
# Activate (Mac/Linux)
source venv_tracking/bin/activate
# Activate (Windows)
venv_tracking\Scripts\activate

pip install mediapipe opencv-python numpy pandas tensorflow keras flask flask-cors deepface tf-keras
```

Optional R packages (if using R for model training):

```r
install.packages("randomForest")
install.packages("keras")
```

#### ğŸ”¹ Terminal 2 â€“ RASA

```bash
py -3.10 -m venv venv_rasa
# Activate (Mac/Linux)
source venv_rasa/bin/activate
# Activate (Windows)
venv_rasa\Scripts\activate

pip install rasa pyttsx3 SpeechRecognition pyaudio
```
---

## ğŸ”„ Quick Start

In a new terminal, not inside any virtual environment, Run everything with:

```bash
python run_all.py
```

This script sets up and launches all required services using the appropriate virtual environments.

---

## ğŸ¥ How It Works

### ğŸ“· Step 1: Capture Video Input

```python
import cv2

cap = cv2.VideoCapture(0)
while cap.isOpened():
    ret, frame = cap.read()
    cv2.imshow("ASL Camera Feed", frame)
    if cv2.waitKey(1) & 0xFF == ord("q"):
        break

cap.release()
cv2.destroyAllWindows()
```

### âœ‹ Step 2: Extract Hand & Pose Keypoints (MediaPipe)

```python
import mediapipe as mp

mp_hands = mp.solutions.hands
hands = mp_hands.Hands()
```

### ğŸ§  Step 3: Train AI Model

In **R**:

```r
library(randomForest)
data <- read.csv("asl_dataset.csv")
model <- randomForest(label ~ ., data = data, ntree = 200)
saveRDS(model, "asl_rf_model.rds")
```

In **Python**:

```python
from tensorflow.keras.models import Sequential

model = Sequential([...])
model.fit(X_train, y_train, epochs=50, batch_size=16)
model.save("asl_model.h5")
```

---

## ğŸ Debug Mode (4-Terminal Setup)

1. **Terminal 1 â€“ Hand Tracking:**

   ```bash
   python code/hand_tracking.py
   ```

2. **Terminal 2 â€“ RASA Actions:**

   ```bash
   rasa run actions
   ```

3. **Terminal 3 â€“ RASA Server:**

   ```bash
   rasa run --enable-api --cors "*" --debug
   ```

4. **Terminal 4 â€“ Speech Handler:**

   ```bash
   python handlers/speech_handler.py
   ```

To retrain intents:

```bash
rasa train
```

Try sample prompts:

```
> What gesture is this?
> How am I feeling?
> Start the Alphabet quiz
> Shall we play a game?
```

---

## ğŸ“Œ Challenges & Future Improvements

- âŒ Improve ASL recognition accuracy with a larger dataset  
- âŒ Implement sentence-level sign translation  
- âŒ Optimize real-time AR performance for mobile/edge devices  

---

## ğŸ’¡ Contributors

| Name                  | Role(s)                                                          |
|-----------------------|------------------------------------------------------------------|
| **Emma Davidson**     | AI/ML Specialist â€¢ Unity Developer â€¢ RASA Developer             |
| **Annie O'Boyle**     | Essay Writer â€¢ Documentation                                     |
| **Neil**              | Dialogue Management â€¢ RASA Integration                           |
| **Sarah Jade Ruthven**| Unity Developer â€¢ Facial Recognition â€¢ Sign Animation            |

---

## ğŸ“œ License

This project is licensed under the MIT License.

---

## ğŸŒŸ Final Thoughts

Letâ€™s break communication barriers with the power of AI and AR.  
**Real-time. Accessible. Inclusive.**
