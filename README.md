# BSL Interpreter for AR Glasses

## ğŸ“Œ Project Overview
This project aims to develop a **British Sign Language (BSL) Interpreter** that runs on **AR glasses**. The system captures real-time **BSL gestures**, translates them into **text**, and displays captions on **augmented reality (AR) glasses**.

## ğŸš€ Features
âœ… **Real-time BSL recognition** using hand and body tracking
âœ… **Machine learning-powered sign language translation**
âœ… **Text overlay in AR glasses** for live captions
âœ… **Optimized for low-latency performance**

## ğŸ“‚ Project Structure
```
ğŸ“ BSL-Interpreter-AR
â”‚â”€â”€ ğŸ“‚ data            # BSL dataset & extracted keypoints
â”‚â”€â”€ ğŸ“‚ models          # Trained machine learning models
â”‚â”€â”€ ğŸ“‚ scripts         # Python & R scripts for processing
â”‚â”€â”€ ğŸ“‚ ar_display      # AR overlay for captions (Unity/OpenCV)
â”‚â”€â”€ README.md         # Project documentation
```

## ğŸ› ï¸ Installation
### 1ï¸âƒ£ Install Dependencies
Make sure you have **Python 3.7 - 3.10** installed. Then, install required libraries:
```bash
pip install mediapipe opencv-python numpy pandas tensorflow keras
```
If using R for training:
```r
install.packages("randomForest")
install.packages("keras")
```

### 2ï¸âƒ£ Setup Virtual Environment (Recommended)
```bash
python -m venv bsl_env
source bsl_env/bin/activate  # Mac/Linux
bsl_env\Scripts\activate     # Windows
```

### 3ï¸âƒ£ Clone the Repository
```bash
git clone https://github.com/your-repo/BSL-Interpreter-AR.git
cd BSL-Interpreter-AR
```

## ğŸ¥ How It Works
### Step 1: Capture Video Input
The AR glasses camera records the **signerâ€™s gestures** in real-time.
```python
import cv2
cap = cv2.VideoCapture(0)
while cap.isOpened():
    ret, frame = cap.read()
    cv2.imshow("BSL Camera Feed", frame)
    if cv2.waitKey(1) & 0xFF == ord("q"): break
cap.release()
cv2.destroyAllWindows()
```

### Step 2: Extract Hand & Pose Keypoints
Using **Mediapipe**, we track **hand and body movement**.
```python
import mediapipe as mp
mp_hands = mp.solutions.hands
hands = mp_hands.Hands()
```

### Step 3: Train AI Model (Using R or Python)
#### Train a Random Forest Model in R
```r
library(randomForest)
data <- read.csv("bsl_dataset.csv")
model <- randomForest(label ~ ., data=data, ntree=200)
saveRDS(model, "bsl_rf_model.rds")
```
#### Train a Deep Learning Model in Python
```python
from tensorflow.keras.models import Sequential
model = Sequential([...])
model.fit(X_train, y_train, epochs=50, batch_size=16)
model.save("bsl_model.h5")
```

### Step 4: Real-Time Captioning in AR
Overlay text captions on **AR glasses** using OpenCV or Unity.
```python
cv2.putText(frame, "Hello!", (50, 50), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)
```

## ğŸ“Œ Challenges & Future Improvements
âŒ Improve **BSL recognition accuracy** with a larger dataset
âŒ Implement **sentence-level sign translation**
âŒ Optimize for **real-time AR processing**

## ğŸ’¡ Contributors
- **Emma Davidson** - Developer
- **Jade Ruthven** - ASL Signer & UREAL Developer

## ğŸ“œ License
This project is licensed under the **MIT License**.

---
ğŸŒŸ **Letâ€™s break communication barriers with AI & AR!** ğŸš€

